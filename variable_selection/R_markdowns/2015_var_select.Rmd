---
title: "2015 Variable Selection"
author: "Konner Macias"
date: "May 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
df <- read.csv("mega-2015.csv", stringsAsFactors = FALSE)
df <- df[-c(363,367),]
row.names(df) <- 1:nrow(df)
```
Look at column names
```{r}
colnames(df)
```
```{r}
library(corrplot)
M <- cor(df[-c(1,2,42)]) # exclude names, pos, and year for correlation
corrplot(M,method = "circle", type = "upper")
```

Variables that correlate the most with average salary
```{r}
sort(M[38,],decreasing = TRUE)
```

FG per game, Points per game, and FGA attempts per game correlate the most.

```{r}
library(leaps)
library(car)
bm <- regsubsets(average.salary ~ FG + PS.G + FGA + MP + FTA + FT + WS + VORP + TOV + OWS + DRB + GS + BPM + OBPM + DWS + USG. + TRB + STL + WS.48 + AST + PF + ORB + TS. + AST. + BLK + FG. + G + DRB. + eFG. + FTr + DBPM + TRB. + FT. + STL. + ORB. + BLK. + TOV., data = df, nbest=1, nvmax=NULL, force.in=NULL, force.out=NULL,method="exhaustive")
bm.sum <- summary(bm)
res.legend <-subsets(bm, statistic="adjr2", legend = FALSE, min.size = 3, max.size = 12, main = "Adjusted R^2")
```
```{r}
which(bm.sum$which[10,] == TRUE)
```

Let's create a linear model and understand the vif of each variable.
```{r}
m1 <- lm(average.salary ~ PS.G + DRB + DWS + STL + AST + PF + AST. + DBPM + STL. + BLK., data = df)
summary(m1)
vif(m1)
```

Can't use this model, the VIF values are insane. We have to use a fewer number of variables.
```{r}
which(bm.sum$which[9,] == TRUE)
```
```{r}
which(bm.sum$which[8,] == TRUE)
```

```{r}
m2 <- lm(average.salary ~ PS.G + DRB + DWS + STL + AST + PF + DBPM + BLK., data = df)
summary(m2)
vif(m2)
```
```{r}
par(mfrow=c(2,2))
plot(m2)
```
Assumption of Normality appears to be violated.


# final 8 variables:
PS.G, DRB, DWS, STL, AST, PF, DBPM, BLK.


Let's rethink average salary.
```{r}
plot(density(df$average.salary,bw="SJ",kern="gaussian"),type="l",main="Gaussian
kernel density estimate",xlab="Average Salary")
```
```{r}
qqnorm(df$average.salary, ylab = "Average Salary")
qqline(df$average.salary, lty = 2, col = 2)
```



Log transformation
```{r}
qqnorm(log(df$average.salary), ylab = "Average Salary")
qqline(log(df$average.salary), lty = 2, col = 2)
```
```{r}
plot(density(log(df$average.salary),bw="SJ",kern="gaussian"),type="l",main="Gaussian
kernel density estimate",xlab="Average Salary")
```


```{r}
bm2 <- regsubsets(log(average.salary) ~ FG + PS.G + FGA + MP + FTA + FT + WS + VORP + TOV + OWS + DRB + GS + BPM + OBPM + DWS + USG. + TRB + STL + WS.48 + AST + PF + ORB + TS. + AST. + BLK + FG. + G + DRB. + eFG. + FTr + DBPM + TRB. + FT. + STL. + ORB. + BLK. + TOV., data = df, nbest=1, nvmax=NULL, force.in=NULL, force.out=NULL,method="exhaustive")
bm2.sum <- summary(bm2)
res.legend <-subsets(bm2, statistic="adjr2", legend = FALSE, min.size = 3, max.size = 12, main = "Adjusted R^2")
```

Go until VIF isn't awful
```{r}
which(bm2.sum$which[5,] == TRUE)
```
```{r}
n1 <- lm(log(average.salary) ~ PS.G + AST + DBPM + STL. + BLK., data = df)
summary(n1)
vif(n1)
```
LOW R^2, VIFs are sound. Expand on STL.

Let's check the diagnostics.
```{r}
par(mfrow=c(2,2))
plot(n1)
```

Pretty good. Let's check with prof.

# final variables in transformed model
PS.G = Points Per Game
AST = Assists Per Game
DBPM = Defensive Box Plus Minus
STL. = Steal Percentage
BLK. = Block Percentage

```{r}
library(ggplot2)
# prepare a special xlab with the number of obs for each group
my_xlab <- paste(levels(as.factor(df$Pos)))
 
# plot
ggplot(df, aes(x=Pos, y=average.salary, fill=Pos)) +
    geom_boxplot(varwidth = TRUE, alpha=0.2) +
    theme(legend.position="none") +
    scale_x_discrete(labels=my_xlab)
```
```{r}
m1.aov <- aov(average.salary ~ as.factor(Pos), data = df)
summary(m1.aov)
```

No significant different in average salaries by position. TODO: Combine positions

Let's standardize our data MP -> VORP
```{r}
scaled.df <- scale(df[5:39])
colMeans(scaled.df)
apply(scaled.df,2, sd)
```
```{r}
std.df <- df
std.df[5:39] <- scaled.df
```
```{r}
write.csv(std.df, file = 'std-2015.csv')
```

```{r}
library(corrplot)
M <- cor(std.df[-c(1,2)]) # exclude names of player for correlation
corrplot(M,method = "circle", type = "upper")
```

```{r}
sort(M[38,],decreasing = TRUE)
```

```{r}
bm3 <- regsubsets(average.salary ~ FG + PS.G + FGA + MP + FTA + FT + WS + VORP + TOV + OWS + DRB + GS + BPM + OBPM + DWS + USG. + TRB + STL + WS.48 + AST + PF + ORB + TS. + AST. + BLK + FG. + G + DRB. + eFG. + FTr + DBPM + TRB. + FT. + STL. + ORB. + BLK. + TOV., data = std.df, nbest=1, nvmax=NULL, force.in=NULL, force.out=NULL,method="exhaustive")
bm3.sum <- summary(bm3)
res.legend <-subsets(bm3, statistic="adjr2", legend = FALSE, min.size = 3, max.size = 12, main = "Adjusted R^2")
```
```{r}
which(bm3.sum$which[5,] == TRUE)
```
```{r}
std.m1 <- lm(average.salary ~ PS.G + DRB + STL + AST +  PF, data = std.df)
summary(std.m1)
vif(std.m1)
```

```{r}
par(mfrow=c(2,2))
plot(std.m1)
```
Assumption of normality is not satisfied.


Apply log transform
```{r}
bm4 <- regsubsets(log(average.salary) ~ FG + PS.G + FGA + MP + FTA + FT + WS + VORP + TOV + OWS + DRB + GS + BPM + OBPM + DWS + USG. + TRB + STL + WS.48 + AST + PF + ORB + TS. + AST. + BLK + FG. + G + DRB. + eFG. + FTr + DBPM + TRB. + FT. + STL. + ORB. + BLK. + TOV., data = std.df, nbest=1, nvmax=NULL, force.in=NULL, force.out=NULL,method="exhaustive")
bm4.sum <- summary(bm4)
res.legend <-subsets(bm4, statistic="adjr2", legend = FALSE, min.size = 3, max.size = 12, main = "Adjusted R^2")
```

```{r}
which(bm4.sum$which[5,] == TRUE)
```

```{r}
std.m2 <- lm(log(average.salary) ~ PS.G + AST + DBPM + STL. + BLK., data = std.df)
summary(std.m2)
vif(std.m2)
```
```{r}
par(mfrow=c(2,2))
plot(std.m2)
```
TODO: transform Y, not x's


That's pretty good.

# Signficant variables:
PS.G
AST
DBPM
STL.
BLK.


STEPWISE AIC and BIC
```{r}
mint <- lm(average.salary~1,data=std.df)
forwardAIC <- step(mint,scope=list(lower=~1,
upper=~FG + PS.G + FGA + MP + FTA + FT + WS + VORP + TOV + OWS + DRB + GS + BPM + OBPM + DWS + USG. + TRB + STL + WS.48 + AST + PF + ORB + TS. + AST. + BLK + FG. + G + DRB. + eFG. + FTr + DBPM + TRB. + FT. + STL. + ORB. + BLK. + TOV.),
direction="forward", data=std.df)
```
```{r}
fwdAIC <- lm(average.salary ~ FG + TOV + STL + VORP + G + USG. + PF + DRB + BLK + AST + AST. + STL. + WS + FTA, data = std.df)
summary(fwdAIC)
vif(fwdAIC)
```
```{r}
n = length(std.df$average.salary)
forwardBIC <- step(mint,scope=list(lower=~1,
upper=~FG + PS.G + FGA + MP + FTA + FT + WS + VORP + TOV + OWS + DRB + GS + BPM + OBPM + DWS + USG. + TRB + STL + WS.48 + AST + PF + ORB + TS. + AST. + BLK + FG. + G + DRB. + eFG. + FTr + DBPM + TRB. + FT. + STL. + ORB. + BLK. + TOV.),
direction="forward", data=std.df,k=log(n))
```

```{r}
forwardBICmodel <- lm(average.salary ~ FG + TOV + STL + VORP + G, data = std.df)
summary(forwardBICmodel)
vif(forwardBICmodel)
```

```{r}
anova(forwardBICmodel, fwdAIC)
```
It is significant to add these variables. So fwdAIC is better.


```{r}
jumbo <- lm(average.salary ~ FG + PS.G + FGA + MP + FTA + FT + WS + VORP + TOV + OWS + DRB + GS + BPM + OBPM + DWS + USG. + TRB + STL + WS.48 + AST + PF + ORB + TS. + AST. + BLK + FG. + G + DRB. + eFG. + FTr + DBPM + TRB. + FT. + STL. + ORB. + BLK. + TOV. , data = std.df)
summary(jumbo)
inverseResponsePlot(jumbo)
summary(powerTransform(jumbo))
```

```{r}
plot(density(log(df$average.salary),bw="SJ",kern="gaussian"),type="l",main="Gaussian
kernel density estimate",xlab="Average Salary")
```



```{r}
n = length(std.df$average.salary)
backBIC <- step(jumbo,direction="backward", data=std.df, k=log(n))
```
```{r}
backBICmodel <- lm(average.salary ~ FG + DRB + DWS + STL + AST + PF + AST. + DBPM + BLK., data=std.df)
summary(backBICmodel)
vif(backBICmodel)
```
```{r}
backAIC <- step(jumbo, direction="backward", data=std.df)
```

```{r}
backAICmodel <- lm(average.salary ~ FG + TOV + DRB + STL + AST + PF + AST. + DBPM + STL. + BLK., data = std.df)
summary(backAICmodel)
vif(backAICmodel)
```

```{r}
anova(backBICmodel,backAICmodel)
```

The addition of one more variable was not significant. So, backBIC is better.


```{r}
anova(backBICmodel, fwdAIC)
```

fwdAIC wins out.


```{r}
summary(inverseResponsePlot())
```

